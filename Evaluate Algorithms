## Evaluate Algorithms ##

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error

train = pipeline(features_train.copy())
test = pipeline(features_test.copy())

## Cross_Validation
from sklearn.model_selection import cross_val_score

def model_selector(estimator):
    squred_error = cross_val_score(estimator, train, label_train, scoring="neg_mean_squared_error", cv=10)
    sqrt_error = np.sqrt(-squred_error)
    mean_error = sqrt_error.mean()
    std_error = sqrt_error.std()
    return mean_error, std_error

estimators = [LinearRegression, DecisionTreeRegressor, RandomForestRegressor, XGBRegressor]
means = []
stds = []
for i in range(len(estimators)):
    estimator = estimators[i]
    mean, std = model_selector(estimator())
    means.append(mean)
    stds.append(std)

set = [means,stds]
set_array = np.array(set)
set_frame = pd.DataFrame(set_array, columns=["LinearRegression", "DecisionTreeRegressor", "RandomForestRegressor", "XGBRegressor"], index= ["mean", "std"])

## Hyperparameter Optimization
from sklearn.model_selection import GridSearchCV
param_grid = [{"n_estimators":[100, 500, 1000, 2000], "max_depth":[3,6,9]}]
estimator = XGBRegressor()
grid_search = GridSearchCV(estimator= estimator, param_grid= param_grid, scoring= "neg_mean_squared_error", cv= 5)
grid_search.fit(train, label_train)

## Select the best algorithm
print("best_parametere =", grid_search.best_params_)

## Test the model on test set
best_estimator = grid_search.best_estimator_
prediction = best_estimator.predict(test)
mse = mean_squared_error(label_test, prediction)
sqrt = np.sqrt(mse)
score = best_estimator.score(test, label_test)
print(sqrt,score)
